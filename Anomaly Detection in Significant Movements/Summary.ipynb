{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of work done\n",
    "\n",
    "Note that this notebook summarises the work done *after* all the work done for the Movement Clustering project. If you are unfamiliar with it, it might be worth reading Summary.ipnyb under 'Clustering Movement Data'.\n",
    "\n",
    "The notation used here will be as follows:\n",
    "\n",
    "Acceleration: $a_i$, where $i$ indicates the degrees of freedom. The Einstein convention is used here, so, the dot product (total acceleration) for example: $a_ia_i = a_1a_1+a_2a_2+a_3a_3$ \n",
    "\n",
    "Gyration: $\\omega_i$\n",
    "\n",
    "Time (continuous): $t$ in units of seconds\n",
    "\n",
    "Time (discontinuous): $\\hat{t}$ in units of seconds\n",
    "\n",
    "The difference between the two different times is that one is representative of *real time*, so when plotted, where there are gaps in the data, there will be gaps in the plot. The second time makes more sense in terms of the *index* of the dataframe that it represents. So that if you have two times, $\\hat{t}_1 = 1$ and $\\hat{t}_2 = 2$, then $\\neg \\Box(\\hat{t}_2 - \\hat{t}_1 = 1)$, since the actual time difference between them could be any arbitrary value.\n",
    "\n",
    "Below is an example:\n",
    "\n",
    "The first plot is gyration vs. real time (from data generated by Rohan). Note that there are many gaps in the data, this is because either some of that data has been filtered, or because the measurements were made at different times.\n",
    "\n",
    "The second plot is gyratino vs. discontinuous time (from the same data set). Notice how all the gaps have been removed.\n",
    "\n",
    "<img style=\"height:300px\" src=\"Images/Rohan_realtime.png\">\n",
    "\n",
    "<img style=\"height:300px\" src=\"Images/Rohan_faketime.png\">\n",
    "\n",
    "Notice that the data sparsity and the small volume make it difficult for forecasting methods that *understand* the time DoF to be used, since predictions from it would be, in effect, meaningless. Another question that is raised is that due to the fact that there isn't that much data, machine learning models used might not be robust, since they would only understand a limited dataset (this wasn't a problem with clustering, due to the fair assumption that 'insignificant' movements are likely to be uniform for all users). That said, a couple of possible machine learning models will be discussed.\n",
    "\n",
    "It's important to be mindful of the **ultimate** goal of this project. At the end, the deployed model should ideally be able to do the following:\n",
    "1. Detect when the movements are lower \"on average\" (this is analogous to 'overflow' from the fluid detection module, i.e. a tap that is left open and thus the flow is higher, on average)\n",
    "2. Adapt to the fact that the average movements will be decreasing as a function of time (because the person is getting older, and their movements less vigorous)\n",
    "3. Detect that there is an overall decrease in daily activity (as measured by how 'strong' the activity is, how frequent naps are -- this links to 2.)\n",
    "4. Detect that there is a high lack of motion (i.e. if the person removed their device or is critically ill), and detect the opposite, when there is a high amount of motion that is abnormal (if the device is attached to a dog)\n",
    "5. Detect irregular movements (i.e. movements that don't have the same pattern that one would expect)\n",
    "\n",
    "To be able to achieve all of these, it is likely that a combination of models may have to be used. For now, this project focuses on detecting when the movements are lower \"on average\" within a pre-defined time window, for example: 5 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding time difference between different datapoints\n",
    "\n",
    "For data exploration purposes, a class was created to visually inspect the seasonality of the data. The class works well for monthly and daily seasonality, but needs some refinement for subdaily (or custom time) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving average\n",
    "\n",
    "Seeing as a suitable machine learning model was difficult to find, it was decided that a moving average would be used for the time being. This moving average would calculate the the average for a pre-specified time window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ideas\n",
    "\n",
    "1. One idea was to calculate the power of the signal for a certain time-frame and compare it to previous powers (analogous to using the moving average method, but with signal power). The advantage that this method *may* have over the moving average is that it can capture a lack of data too. So if for the pre-specified time-frame, 90% of the data is 0 (because say the elder is napping), then it would capture that in how 'powerful' the signal is. \n",
    "2. Other ideas?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KinKeepers_AI",
   "language": "python",
   "name": "kinkeepers_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
